{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CLUSTERING ASSIGNMENT"
      ],
      "metadata": {
        "id": "j4MdcLyZGGIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the difference between K-Means and Hierarchical Clustering?\n",
        "Provide a use case for each.\n",
        "ANSWER -üîπ Difference between K-Means and Hierarchical Clustering\n",
        "Aspect\tK-Means Clustering\tHierarchical Clustering\n",
        "Approach\tPartition-based: Divides data into k clusters by minimizing distance to cluster centroids.\tTree-based: Builds a hierarchy of clusters using agglomerative (bottom-up) or divisive (top-down) approaches.\n",
        "Number of Clusters\tMust be specified in advance (k).\tNo need to predefine clusters ‚Äî you can choose clusters by cutting the dendrogram at a desired level.\n",
        "Scalability\tWorks well with large datasets (fast and efficient).\tComputationally expensive (O(n¬≤)), better for small to medium datasets.\n",
        "Cluster Shape\tAssumes spherical clusters (works best when clusters are well-separated).\tCan capture complex, nested cluster structures.\n",
        "Result\tProduces flat clusters.\tProduces a dendrogram (tree structure of clusters).\n",
        "Stability\tRandom initialization may lead to different results.\tMore stable since it does not rely on random initialization.\n",
        " Use Cases\n",
        "\n",
        "K-Means Use Case:\n",
        "Customer segmentation in e-commerce.\n",
        "‚Üí For example, grouping customers into k segments based on purchase history, spending habits, and demographics for targeted marketing.\n",
        "\n",
        "Hierarchical Clustering Use Case:\n",
        "Gene expression analysis in bioinformatics.\n",
        "‚Üí Researchers use hierarchical clustering to group genes or proteins with similar expression patterns and visualize relationships through a dendrogram."
      ],
      "metadata": {
        "id": "PsNsZfFtGNWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the purpose of the Silhouette Score in evaluating clustering\n",
        "algorithms.\n",
        "ANSWER - üîπ Purpose of Silhouette Score\n",
        "\n",
        "The Silhouette Score is a metric used to evaluate the quality of clusters created by a clustering algorithm (like K-Means, DBSCAN, or Hierarchical Clustering).\n",
        "\n",
        "It measures how well each data point fits into its assigned cluster compared to other clusters.\n",
        "\n",
        "üîπ Formula\n",
        "\n",
        "For a data point\n",
        "ùëñ\n",
        "i:\n",
        "\n",
        "ùë†\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "=\n",
        "ùëè\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "‚àí\n",
        "ùëé\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "max\n",
        "‚Å°\n",
        "(\n",
        "ùëé\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ",\n",
        "ùëè\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "s(i)=\n",
        "max(a(i),b(i))\n",
        "b(i)‚àía(i)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëé\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "a(i) = Average distance of point\n",
        "ùëñ\n",
        "i to all other points in its own cluster (cohesion).\n",
        "\n",
        "ùëè\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "b(i) = Average distance of point\n",
        "ùëñ\n",
        "i to points in the nearest neighboring cluster (separation).\n",
        "\n",
        "üîπ Interpretation\n",
        "\n",
        "+1 (close to 1): Point is well-clustered ‚Üí far from other clusters and close to its own cluster.\n",
        "\n",
        "0: Point is on or near the boundary between clusters.\n",
        "\n",
        "-1 (negative): Point is likely in the wrong cluster (closer to another cluster than its own).\n",
        "\n",
        "üîπ Why it‚Äôs useful\n",
        "\n",
        "Helps choose the optimal number of clusters (higher average silhouette score indicates better clustering).\n",
        "\n",
        "Works without ground truth labels ‚Üí perfect for unsupervised learning evaluation.\n",
        "\n",
        "Gives both global and individual quality: you can check overall cluster validity and also detect misclassified points."
      ],
      "metadata": {
        "id": "00bfc-fOGtO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What are the core parameters of DBSCAN, and how do they influence the clustering process?\n",
        "ANSWER- üîπ Core Parameters of DBSCAN\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) mainly relies on two parameters:\n",
        "\n",
        "Œµ (Epsilon / eps)\n",
        "\n",
        "Defines the radius of the neighborhood around a point.\n",
        "\n",
        "Two points are considered neighbors if the distance between them is ‚â§ Œµ.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Small Œµ ‚Üí many points become outliers, clusters may fragment.\n",
        "\n",
        "Large Œµ ‚Üí clusters may merge, reducing the number of clusters.\n",
        "\n",
        "minPts (Minimum Points)\n",
        "\n",
        "The minimum number of points required to form a dense region (a cluster).\n",
        "\n",
        "Includes the point itself + neighbors within Œµ.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Low minPts ‚Üí even sparse areas become clusters (more noise absorbed).\n",
        "\n",
        "High minPts ‚Üí requires denser regions, more points labeled as noise.\n",
        "\n",
        "üîπ Types of Points in DBSCAN\n",
        "\n",
        "Core Point: Has at least minPts points within Œµ.\n",
        "\n",
        "Border Point: Has fewer than minPts neighbors, but lies within the neighborhood of a core point.\n",
        "\n",
        "Noise (Outlier): Not a core point and not in any core point‚Äôs neighborhood.\n",
        "\n",
        "üîπ How They Influence Clustering\n",
        "\n",
        "Œµ controls \"how close is close enough\".\n",
        "\n",
        "minPts controls \"how many neighbors make a dense cluster\".\n",
        "Together, they determine:\n",
        "\n",
        "Cluster shape (DBSCAN can find arbitrary-shaped clusters, unlike K-Means).\n",
        "\n",
        "How much noise/outliers are identified.\n"
      ],
      "metadata": {
        "id": "00AD7dnpHF3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is feature scaling important when applying clustering algorithms like K-Means and DBSCAN?\n",
        "ANSWER - üîπ Why Feature Scaling Matters in Clustering\n",
        "\n",
        "Clustering algorithms like K-Means and DBSCAN rely heavily on distance measures (usually Euclidean distance) to group points.\n",
        "\n",
        "üëâ If features are on very different scales, the larger-scaled features will dominate the distance calculation, and clustering results will be biased.\n",
        "\n",
        "üîπ Example\n",
        "\n",
        "Suppose you‚Äôre clustering customers using:\n",
        "\n",
        "Age: 20‚Äì70 (small range)\n",
        "\n",
        "Annual Income: 30,000‚Äì200,000 (large range)\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "Differences in income dominate the distance metric.\n",
        "\n",
        "The algorithm may completely ignore age in forming clusters.\n",
        "\n",
        "üîπ Impact on Algorithms\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Updates centroids using distances.\n",
        "\n",
        "If one feature has a larger scale, clusters will be skewed toward that feature.\n",
        "\n",
        "DBSCAN:\n",
        "\n",
        "Uses Œµ-radius neighborhood.\n",
        "\n",
        "If one feature dominates, Œµ becomes meaningless ‚Äî clusters may not form properly.\n",
        "\n",
        "üîπ Scaling Techniques Commonly Used\n",
        "\n",
        "Standardization (Z-score):\n",
        "\n",
        "ùë•\n",
        "‚Ä≤\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "x\n",
        "‚Ä≤\n",
        "=\n",
        "œÉ\n",
        "x‚àíŒº\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "‚Üí Centers features around 0 with unit variance.\n",
        "\n",
        "Min-Max Normalization:\n",
        "\n",
        "ùë•\n",
        "‚Ä≤\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "min\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "max\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "‚àí\n",
        "min\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "x\n",
        "‚Ä≤\n",
        "=\n",
        "max(x)‚àímin(x)\n",
        "x‚àímin(x)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "‚Üí Scales features to a fixed range (often 0‚Äì1).\n",
        "\n",
        "Robust Scaling:\n",
        "Uses median and IQR (good for outliers)."
      ],
      "metadata": {
        "id": "nL3td6IIHszK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is the Elbow Method in K-Means clustering and how does it help determine the optimal number of clusters?\n",
        "ANSWER -The Elbow Method is a technique used in K-Means clustering to find the optimal number of clusters (k).\n",
        "\n",
        "It works by looking at how the Within-Cluster Sum of Squares (WCSS), also called inertia, changes as you increase the number of clusters.\n",
        "\n",
        "üîπ Steps of the Elbow Method\n",
        "\n",
        "Run K-Means with different values of\n",
        "ùëò\n",
        "k (e.g., 1 to 10).\n",
        "\n",
        "For each\n",
        "ùëò\n",
        "k, compute the WCSS (inertia):\n",
        "\n",
        "ùëä\n",
        "ùê∂\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "‚àë\n",
        "ùë•\n",
        "‚àà\n",
        "ùê∂\n",
        "ùëñ\n",
        "‚à£\n",
        "‚à£\n",
        "ùë•\n",
        "‚àí\n",
        "ùúá\n",
        "ùëñ\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "WCSS=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "x‚ààC\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àë\n",
        "\t‚Äã\n",
        "\n",
        "‚à£‚à£x‚àíŒº\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£‚à£\n",
        "2\n",
        "\n",
        "where\n",
        "ùê∂\n",
        "ùëñ\n",
        "C\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " is cluster\n",
        "ùëñ\n",
        "i, and\n",
        "ùúá\n",
        "ùëñ\n",
        "Œº\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " is its centroid.\n",
        "\n",
        "Plot WCSS vs. number of clusters (k).\n",
        "\n",
        "Look for the point where the rate of decrease sharply slows down ‚Äî this point looks like an ‚Äúelbow‚Äù in the curve."
      ],
      "metadata": {
        "id": "VZ3of6j0INSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 6: Generate synthetic data using make_blobs(n_samples=300, centers=4),apply KMeans clustering, and visualize the results with cluster centers.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 3: Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "# Plot cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Centers')\n",
        "\n",
        "plt.title(\"K-Means Clustering with 4 Centers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print cluster centers\n",
        "print(\"Cluster Centers:\\n\", centers)"
      ],
      "metadata": {
        "id": "OOuipDGQIzY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - [[ 4.6869   2.0143 ]\n",
        " [-2.6052   8.9928 ]\n",
        " [-6.8513  -6.8503 ]\n",
        " [-8.8346   7.2443 ]]"
      ],
      "metadata": {
        "id": "RaPLwRa-Jmpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 7: Load the Wine dataset, apply StandardScaler , and then train a DBSCAN model. Print the number of clusters found (excluding noise).\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Step 2: Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Train DBSCAN model\n",
        "dbscan = DBSCAN(eps=1.8, min_samples=5)  # eps tuned for wine dataset\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Step 4: Count clusters (excluding noise = label -1)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "print(\"Number of clusters found (excluding noise):\", n_clusters)\n",
        "print(\"Cluster labels:\", np.unique(labels))"
      ],
      "metadata": {
        "id": "1q0RKKkgJso0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT -Number of clusters found (excluding noise): 7\n",
        "Cluster labels: [-1  0  1  2  3  4  5  6]"
      ],
      "metadata": {
        "id": "xNhEhCqDKGMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 8: Generate moon-shaped synthetic data using make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in the plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate moon-shaped synthetic data\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot clusters and highlight outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot clustered points\n",
        "plt.scatter(X[labels >= 0, 0], X[labels >= 0, 1],\n",
        "            c=labels[labels >= 0], cmap='viridis', s=50, label=\"Clusters\")\n",
        "\n",
        "# Highlight outliers (label = -1)\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1],\n",
        "            c='red', s=70, marker='x', label=\"Outliers\")\n",
        "\n",
        "plt.title(\"DBSCAN on Moon-shaped Data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print unique labels for clarity\n",
        "print(\"Cluster labels:\", np.unique(labels))"
      ],
      "metadata": {
        "id": "hk7xI9TbKN5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Cluster labels: [-1  0  1]"
      ],
      "metadata": {
        "id": "rihNOcbdKzTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 9: Load the Wine dataset, reduce it to 2D using PCA, then apply Agglomerative Clustering and visualize the result in 2D with a scatter plot.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Step 2: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Reduce dimensions to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 4: Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)  # wine has 3 classes\n",
        "labels = agg_clustering.fit_predict(X_pca)\n",
        "\n",
        "# Step 5: Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.title(\"Agglomerative Clustering on Wine Dataset (PCA-reduced 2D)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Print cluster label summary\n",
        "print(\"Unique cluster labels:\", set(labels))"
      ],
      "metadata": {
        "id": "v7SuYvKoK5Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT -Unique cluster labels: {0, 1, 2}"
      ],
      "metadata": {
        "id": "6ojSGfjULYRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 10: You are working as a data analyst at an e-commerce company. The marketing team wants to segment customers based on their purchasing behavior to run\n",
        "targeted promotions. The dataset contains customer demographics and their product purchase history across categories.\n",
        "Describe your real-world data science workflow using clustering:\n",
        "‚óè Which clustering algorithm(s) would you use and why?\n",
        "‚óè How would you preprocess the data (missing values, scaling)?\n",
        "‚óè How would you determine the number of clusters?\n",
        "‚óè How would the marketing team benefit from your clustering analysis?\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Step 1: Simulate customer data (demographics + purchase behavior)\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.2, random_state=42)\n",
        "\n",
        "# Step 2: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Determine optimal clusters (Elbow + Silhouette)\n",
        "wcss = []\n",
        "sil_scores = []\n",
        "K = range(2, 8)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    sil_scores.append(silhouette_score(X_scaled, labels))\n",
        "\n",
        "# Plot Elbow Method\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(K, wcss, 'bo-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.title('Elbow Method')\n",
        "\n",
        "# Plot Silhouette Score\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(K, sil_scores, 'go-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Apply KMeans with optimal k (say 4)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Step 5: Visualize clusters\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_scaled[:,0], X_scaled[:,1], c=labels, cmap='viridis', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],\n",
        "            c='red', marker='X', s=200, label='Centers')\n",
        "plt.title(\"Customer Segmentation using K-Means\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print results\n",
        "print(\"Optimal number of clusters chosen:\", 4)\n",
        "print(\"Cluster labels distribution:\\n\", pd.Series(labels).value_counts())"
      ],
      "metadata": {
        "id": "uG0MTdiYLeGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Elbow Plot ‚Üí Shows ‚Äúelbow‚Äù around k=4.\n",
        "\n",
        "Silhouette Score Plot ‚Üí Peak around k=4.\n",
        "\n",
        "Cluster Visualization ‚Üí 4 distinct customer groups with red centers.\n",
        "\n",
        "Cluster distribution in the console (e.g., how many customers per segment)."
      ],
      "metadata": {
        "id": "CQ7nzNU5M6cd"
      }
    }
  ]
}